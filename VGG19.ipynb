{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"VGG19.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1tgqNnwFJtrlmuRFUddI9Z_hRdMX5zD9T","authorship_tag":"ABX9TyPgVjaLAE6w/UgHMkk7fz/M"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"CJ-tqFTTBMLR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607587064886,"user_tz":-420,"elapsed":525079,"user":{"displayName":"Hoang Viet Nguyen","photoUrl":"","userId":"00365099228452340445"}},"outputId":"b2197581-f96d-48a1-b08f-f934c0cba128"},"source":["!pip install kaggle\n","!pip uninstall -y kaggle\n","!pip install --upgrade pip\n","!pip install kaggle==1.5.6\n","!mkdir -p ~/.kaggle\n","!cp \"/content/drive/My Drive/kaggle.json\" ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json\n","!kaggle competitions download -c shopee-product-detection-student\n","!unzip -q shopee-product-detection-student.zip"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.9)\n","Requirement already satisfied: slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (0.0.1)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.11.8)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.10)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n","Uninstalling kaggle-1.5.9:\n","  Successfully uninstalled kaggle-1.5.9\n","Collecting pip\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/11/2dc62c5263d9eb322f2f028f7b56cd9d096bb8988fcf82d65fa2e4057afe/pip-20.3.1-py2.py3-none-any.whl (1.5MB)\n","\u001b[K     |████████████████████████████████| 1.5MB 7.6MB/s \n","\u001b[?25hInstalling collected packages: pip\n","  Found existing installation: pip 19.3.1\n","    Uninstalling pip-19.3.1:\n","      Successfully uninstalled pip-19.3.1\n","Successfully installed pip-20.3.1\n","Collecting kaggle==1.5.6\n","  Downloading kaggle-1.5.6.tar.gz (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 4.6 MB/s \n","\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (1.24.3)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (1.15.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2020.11.8)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2.8.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2.23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (4.41.1)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (4.0.1)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (1.15.0)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle==1.5.6) (1.3)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle==1.5.6) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle==1.5.6) (3.0.4)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2020.11.8)\n","Building wheels for collected packages: kaggle\n","  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kaggle: filename=kaggle-1.5.6-py3-none-any.whl size=72859 sha256=432de7740951695c6788b951d1ff9d197caf3e0f0fe833aeb4fe38734103fef7\n","  Stored in directory: /root/.cache/pip/wheels/01/3e/ff/77407ebac3ef71a79b9166a8382aecf88415a0bcbe3c095a01\n","Successfully built kaggle\n","Installing collected packages: kaggle\n","Successfully installed kaggle-1.5.6\n","Downloading shopee-product-detection-student.zip to /content\n","100% 9.37G/9.38G [03:38<00:00, 87.7MB/s]\n","100% 9.38G/9.38G [03:38<00:00, 46.0MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rov0v52mVYtm","executionInfo":{"status":"ok","timestamp":1608601392129,"user_tz":-420,"elapsed":1725,"user":{"displayName":"Hoang Viet Nguyen","photoUrl":"","userId":"00365099228452340445"}}},"source":["import tensorflow as tf\n","import keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPool2D\n","from tensorflow.keras.layers import Dense, Flatten, Dropout\n","from tensorflow.keras.losses import categorical_crossentropy\n","\n","class VGG19(Sequential):\n","  def __init__(self, input_shape,k):\n","    super().__init__()\n","    self.add(Conv2D(64, kernel_size=(3,3), padding='same', activation='relu', input_shape = input_shape))\n","    self.add(Conv2D(64, kernel_size=(3,3), padding='same', activation='relu'))\n","    self.add(MaxPool2D(pool_size=(2,2), strides = (2,2)))\n","\n","    self.add(Conv2D(128, kernel_size=(3,3), padding='same', activation='relu'))\n","    self.add(Conv2D(128, kernel_size=(3,3), padding='same', activation='relu'))\n","    self.add(MaxPool2D(pool_size=(2,2), strides = (2,2)))\n","\n","    self.add(Conv2D(256, kernel_size=(3,3), padding='same', activation='relu'))\n","    self.add(Conv2D(256, kernel_size=(3,3), padding='same', activation='relu'))\n","    self.add(Conv2D(256, kernel_size=(3,3), padding='same', activation='relu'))\n","    self.add(Conv2D(256, kernel_size=(3,3), padding='same', activation='relu'))\n","    self.add(MaxPool2D(pool_size = (2,2), strides = (2,2)))\n","\n","    self.add(Conv2D(512, kernel_size=(3,3), padding='same', activation='relu'))\n","    self.add(Conv2D(512, kernel_size=(3,3), padding='same', activation='relu'))\n","    self.add(Conv2D(512, kernel_size=(3,3), padding='same', activation='relu'))\n","    self.add(Conv2D(512, kernel_size=(3,3), padding='same', activation='relu'))\n","    self.add(MaxPool2D(pool_size = (2,2), strides = (2,2)))\n","\n","    self.add(Conv2D(512, kernel_size=(3,3), padding='same', activation='relu'))\n","    self.add(Conv2D(512, kernel_size=(3,3), padding='same', activation='relu'))\n","    self.add(Conv2D(512, kernel_size=(3,3), padding='same', activation='relu'))\n","    self.add(Conv2D(512, kernel_size=(3,3), padding='same', activation='relu'))\n","    self.add(MaxPool2D(pool_size = (2,2), strides = (2,2)))\n","\n","    self.add(Flatten())\n","    self.add(Dense(4096, activation='relu'))\n","    self.add(Dropout(0.5))\n","    self.add(Dense(4096, activation='relu'))\n","    self.add(Dropout(0.5))\n","    self.add(Dense(1000, activation='relu'))\n","    self.add(Dropout(0.5))\n","    self.add(Dense(k, activation = 'softmax'))"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"vTtA8g90Vgp8","executionInfo":{"status":"ok","timestamp":1608601354829,"user_tz":-420,"elapsed":3314,"user":{"displayName":"Hoang Viet Nguyen","photoUrl":"","userId":"00365099228452340445"}}},"source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dropout\n","from tensorflow.keras.layers import Activation, BatchNormalization, Add, Reshape, DepthwiseConv2D\n","\n","from tensorflow.keras import backend as K\n","\n","\n","def _make_divisible(v, divisor, min_value=None):\n","    if min_value is None:\n","        min_value = divisor\n","    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n","    if new_v < 0.9 * v:\n","        new_v += divisor\n","    return new_v\n","\n","\n","def relu6(x):\n","    return K.relu(x, max_value=6.0)\n","\n","\n","def _conv_block(inputs, filters, kernel, strides):\n","    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n","\n","    x = Conv2D(filters, kernel, padding='same', strides=strides)(inputs)\n","    x = BatchNormalization(axis=channel_axis)(x)\n","    return Activation(relu6)(x)\n","\n","\n","def _bottleneck(inputs, filters, kernel, t, alpha, s, r=False):\n","    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n","    tchannel = K.int_shape(inputs)[channel_axis] * t\n","    cchannel = int(filters * alpha)\n","\n","    x = _conv_block(inputs, tchannel, (1, 1), (1, 1))\n","\n","    x = DepthwiseConv2D(kernel, strides=(s, s), depth_multiplier=1, padding='same')(x)\n","    x = BatchNormalization(axis=channel_axis)(x)\n","    x = Activation(relu6)(x)\n","\n","    x = Conv2D(cchannel, (1, 1), strides=(1, 1), padding='same')(x)\n","    x = BatchNormalization(axis=channel_axis)(x)\n","\n","    if r:\n","        x = Add()([x, inputs])\n","\n","    return x\n","\n","\n","def _inverted_residual_block(inputs, filters, kernel, t, alpha, strides, n):\n","    x = _bottleneck(inputs, filters, kernel, t, alpha, strides)\n","\n","    for i in range(1, n):\n","        x = _bottleneck(x, filters, kernel, t, alpha, 1, True)\n","\n","    return x\n","\n","\n","def MobileNetv2(input_shape, k, alpha=1.0):\n","    inputs = Input(shape=input_shape)\n","\n","    first_filters = _make_divisible(32 * alpha, 8)\n","    x = _conv_block(inputs, first_filters, (3, 3), strides=(2, 2))\n","\n","    x = _inverted_residual_block(x, 16, (3, 3), t=1, alpha=alpha, strides=1, n=1)\n","    x = _inverted_residual_block(x, 24, (3, 3), t=6, alpha=alpha, strides=2, n=2)\n","    x = _inverted_residual_block(x, 32, (3, 3), t=6, alpha=alpha, strides=2, n=3)\n","    x = _inverted_residual_block(x, 64, (3, 3), t=6, alpha=alpha, strides=2, n=4)\n","    x = _inverted_residual_block(x, 96, (3, 3), t=6, alpha=alpha, strides=1, n=3)\n","    x = _inverted_residual_block(x, 160, (3, 3), t=6, alpha=alpha, strides=2, n=3)\n","    x = _inverted_residual_block(x, 320, (3, 3), t=6, alpha=alpha, strides=1, n=1)\n","\n","    if alpha > 1.0:\n","        last_filters = _make_divisible(1280 * alpha, 8)\n","    else:\n","        last_filters = 1280\n","\n","    x = _conv_block(x, last_filters, (1, 1), strides=(1, 1))\n","    x = GlobalAveragePooling2D()(x)\n","    x = Reshape((1, 1, last_filters))(x)\n","    x = Dropout(0.3, name='Dropout')(x)\n","    x = Conv2D(k, (1, 1), padding='same')(x)\n","\n","    x = Activation('softmax', name='softmax')(x)\n","    output = Reshape((k,))(x)\n","\n","    model = Model(inputs, output)\n","\n","    return model\n","\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"flEk_TmN7cMi"},"source":["model = MobileNetv2((32,32,3),1000)\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(),\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",")\n","print(model.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ArHvwPV_8l_j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607654499534,"user_tz":-420,"elapsed":943577,"user":{"displayName":"Hoang Viet Nguyen","photoUrl":"","userId":"00365099228452340445"}},"outputId":"7fc6b6ba-2b2a-4a09-c229-54a9da03b424"},"source":["import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import keras\n","from tensorflow.keras import datasets, layers, models\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import Model\n","\n","ds_train, ds_test = tfds.load(\n","    'cifar10',\n","    split=['train', 'test'],\n","    shuffle_files=True,\n","    as_supervised=True,\n",")\n","\n","def normalize_img(image, label):\n","  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n","  return tf.cast(image, tf.float32) / 255., label\n","\n","ds_train = ds_train.map(\n","    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","ds_train = ds_train.batch(128)\n","ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n","\n","ds_test = ds_test.map(\n","    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","ds_test = ds_test.batch(128)\n","ds_test = ds_test.cache()\n","ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n","\n","model = tf.keras.applications.MobileNetV2(\n","    input_shape=(32,32,3),\n","    alpha=1.0,\n","    include_top=True,\n","    weights=None,\n","    input_tensor=None,\n","    pooling=None,\n","    classes=10,\n","    classifier_activation=\"softmax\",\n",")\n","\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(),\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",")\n","\n","print(ds_train)\n","print(ds_test)\n","history = model.fit(\n","    ds_train,\n","    epochs=100,\n","    validation_data=ds_test\n",")\n","print(history)\n","\n","model.save('/content/drive/My Drive/Shopee/mobilenetv2-keras-cifar10.hdf5')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<PrefetchDataset shapes: ((None, 32, 32, 3), (None,)), types: (tf.float32, tf.int64)>\n","<PrefetchDataset shapes: ((None, 32, 32, 3), (None,)), types: (tf.float32, tf.int64)>\n","Epoch 1/100\n","391/391 [==============================] - 16s 41ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 2/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 3/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 4/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 5/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 6/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 7/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 8/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 9/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 10/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 11/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 12/100\n","391/391 [==============================] - 10s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 13/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 14/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 15/100\n","391/391 [==============================] - 10s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 16/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 17/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 18/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 19/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 20/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 21/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 22/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 23/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 24/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 25/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 26/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 27/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 28/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 29/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 30/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 31/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 32/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 33/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 34/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 35/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 36/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 37/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 38/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 39/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 40/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 41/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 42/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 43/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 44/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 45/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 46/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 47/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 48/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 49/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 50/100\n","391/391 [==============================] - 10s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 51/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 52/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 53/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 54/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 55/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 56/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 57/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 58/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 59/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 60/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 61/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 62/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 63/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 64/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 65/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 66/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 67/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 68/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 69/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 70/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 71/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 72/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 73/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 74/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 75/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 76/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 77/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 78/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 79/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 80/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 81/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 82/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 83/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 84/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 85/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 86/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 87/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 88/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 89/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 90/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 91/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 92/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 93/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 94/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 95/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 96/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 97/100\n","391/391 [==============================] - 9s 23ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 98/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 99/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","Epoch 100/100\n","391/391 [==============================] - 9s 24ms/step - loss: nan - sparse_categorical_accuracy: 0.0100 - val_loss: nan - val_sparse_categorical_accuracy: 0.0100\n","<tensorflow.python.keras.callbacks.History object at 0x7fc700db3ac8>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rh5faNq6Y0ym","executionInfo":{"status":"ok","timestamp":1607650119182,"user_tz":-420,"elapsed":152502,"user":{"displayName":"Hoang Viet Nguyen","photoUrl":"","userId":"00365099228452340445"}},"outputId":"49e48788-268e-4889-cab0-f56e8ad9e814"},"source":["import tensorflow as tf\r\n","import tensorflow_datasets as tfds\r\n","import keras\r\n","from tensorflow.keras import datasets, layers, models\r\n","import matplotlib.pyplot as plt\r\n","from tensorflow.keras.models import Model\r\n","\r\n","ds_train, ds_test = tfds.load(\r\n","    'cifar10',\r\n","    split=['train', 'test'],\r\n","    shuffle_files=True,\r\n","    as_supervised=True,\r\n",")\r\n","\r\n","def normalize_img(image, label):\r\n","  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\r\n","  return tf.cast(image, tf.float32) / 255., label\r\n","\r\n","ds_train = ds_train.map(\r\n","    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n","ds_train = ds_train.batch(128)\r\n","ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\r\n","\r\n","ds_test = ds_test.map(\r\n","    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n","ds_test = ds_test.batch(128)\r\n","ds_test = ds_test.cache()\r\n","ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\r\n","\r\n","model = VGG19((32,32,3), 10)\r\n","model.compile(\r\n","    optimizer=tf.keras.optimizers.Adam(),\r\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n","    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\r\n",")\r\n","history = model.fit(\r\n","    ds_train,\r\n","    epochs=5,\r\n","    validation_data=ds_test\r\n",")\r\n","print(history)\r\n","\r\n","model.save('/content/drive/My Drive/Shopee/vgg19-cifar10.hdf5')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","  2/391 [..............................] - ETA: 14s - loss: 2.3025 - sparse_categorical_accuracy: 0.1484WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0241s vs `on_train_batch_end` time: 0.0480s). Check your callbacks.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0241s vs `on_train_batch_end` time: 0.0480s). Check your callbacks.\n"],"name":"stderr"},{"output_type":"stream","text":["391/391 [==============================] - 31s 79ms/step - loss: 2.3603 - sparse_categorical_accuracy: 0.0998 - val_loss: 2.3612 - val_sparse_categorical_accuracy: 0.1000\n","Epoch 2/5\n","391/391 [==============================] - 29s 74ms/step - loss: 2.3611 - sparse_categorical_accuracy: 0.1000 - val_loss: 2.3612 - val_sparse_categorical_accuracy: 0.1000\n","Epoch 3/5\n","391/391 [==============================] - 29s 74ms/step - loss: 2.3611 - sparse_categorical_accuracy: 0.1000 - val_loss: 2.3612 - val_sparse_categorical_accuracy: 0.1000\n","Epoch 4/5\n","391/391 [==============================] - 29s 75ms/step - loss: 2.3611 - sparse_categorical_accuracy: 0.1000 - val_loss: 2.3612 - val_sparse_categorical_accuracy: 0.1000\n","Epoch 5/5\n","391/391 [==============================] - 29s 74ms/step - loss: 2.3611 - sparse_categorical_accuracy: 0.1000 - val_loss: 2.3612 - val_sparse_categorical_accuracy: 0.1000\n","<tensorflow.python.keras.callbacks.History object at 0x7fc7a7f2ecf8>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sdOfYXQIY-27","executionInfo":{"status":"ok","timestamp":1607646415572,"user_tz":-420,"elapsed":1108577,"user":{"displayName":"Hoang Viet Nguyen","photoUrl":"","userId":"00365099228452340445"}},"outputId":"f390c235-9273-4752-b8b3-1e15e10f8cc6"},"source":["import tensorflow as tf\r\n","import tensorflow_datasets as tfds\r\n","import keras\r\n","from tensorflow.keras import datasets, layers, models\r\n","import matplotlib.pyplot as plt\r\n","from tensorflow.keras.models import Model\r\n","\r\n","ds_train, ds_test = tfds.load(\r\n","    'kmnist',\r\n","    split=['train', 'test'],\r\n","    shuffle_files=True,\r\n","    as_supervised=True,\r\n",")\r\n","\r\n","def normalize_img(image, label):\r\n","  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\r\n","  return tf.cast(image, tf.float32) / 255., label\r\n","\r\n","ds_train = ds_train.map(\r\n","    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n","ds_train = ds_train.batch(128)\r\n","ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\r\n","\r\n","ds_test = ds_test.map(\r\n","    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n","ds_test = ds_test.batch(128)\r\n","ds_test = ds_test.cache()\r\n","ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\r\n","\r\n","model = MobileNetv2((28,28,1), 10)\r\n","\r\n","model.compile(\r\n","    optimizer=tf.keras.optimizers.Adam(),\r\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n","    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\r\n",")\r\n","\r\n","print(ds_train)\r\n","print(ds_test)\r\n","history = model.fit(\r\n","    ds_train,\r\n","    epochs=100,\r\n","    validation_data=ds_test\r\n",")\r\n","print(history)\r\n","\r\n","model.save('/content/drive/My Drive/Shopee/mobilenetv2-kmnist.hdf5')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<PrefetchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>\n","<PrefetchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>\n","Epoch 1/100\n","469/469 [==============================] - 17s 36ms/step - loss: 1.8598 - sparse_categorical_accuracy: 0.5975 - val_loss: 2.3060 - val_sparse_categorical_accuracy: 0.1000\n","Epoch 2/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.6728 - sparse_categorical_accuracy: 0.7874 - val_loss: 1.8532 - val_sparse_categorical_accuracy: 0.6069\n","Epoch 3/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.6337 - sparse_categorical_accuracy: 0.8272 - val_loss: 1.7813 - val_sparse_categorical_accuracy: 0.6793\n","Epoch 4/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.6103 - sparse_categorical_accuracy: 0.8506 - val_loss: 1.7041 - val_sparse_categorical_accuracy: 0.7571\n","Epoch 5/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5938 - sparse_categorical_accuracy: 0.8672 - val_loss: 1.7701 - val_sparse_categorical_accuracy: 0.6902\n","Epoch 6/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5847 - sparse_categorical_accuracy: 0.8761 - val_loss: 1.7222 - val_sparse_categorical_accuracy: 0.7388\n","Epoch 7/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5771 - sparse_categorical_accuracy: 0.8839 - val_loss: 1.8050 - val_sparse_categorical_accuracy: 0.6561\n","Epoch 8/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5774 - sparse_categorical_accuracy: 0.8838 - val_loss: 1.6892 - val_sparse_categorical_accuracy: 0.7717\n","Epoch 9/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5712 - sparse_categorical_accuracy: 0.8898 - val_loss: 1.6620 - val_sparse_categorical_accuracy: 0.7991\n","Epoch 10/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5641 - sparse_categorical_accuracy: 0.8967 - val_loss: 1.6736 - val_sparse_categorical_accuracy: 0.7872\n","Epoch 11/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5603 - sparse_categorical_accuracy: 0.9007 - val_loss: 1.6474 - val_sparse_categorical_accuracy: 0.8135\n","Epoch 12/100\n","469/469 [==============================] - 11s 22ms/step - loss: 1.5502 - sparse_categorical_accuracy: 0.9110 - val_loss: 1.7799 - val_sparse_categorical_accuracy: 0.6813\n","Epoch 13/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5674 - sparse_categorical_accuracy: 0.8935 - val_loss: 1.7209 - val_sparse_categorical_accuracy: 0.7395\n","Epoch 14/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5567 - sparse_categorical_accuracy: 0.9044 - val_loss: 1.6468 - val_sparse_categorical_accuracy: 0.8139\n","Epoch 15/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5496 - sparse_categorical_accuracy: 0.9116 - val_loss: 1.6656 - val_sparse_categorical_accuracy: 0.7949\n","Epoch 16/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5677 - sparse_categorical_accuracy: 0.8933 - val_loss: 1.7394 - val_sparse_categorical_accuracy: 0.7213\n","Epoch 17/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5780 - sparse_categorical_accuracy: 0.8831 - val_loss: 1.6161 - val_sparse_categorical_accuracy: 0.8449\n","Epoch 18/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5523 - sparse_categorical_accuracy: 0.9087 - val_loss: 1.7096 - val_sparse_categorical_accuracy: 0.7513\n","Epoch 19/100\n","469/469 [==============================] - 10s 22ms/step - loss: 1.5634 - sparse_categorical_accuracy: 0.8976 - val_loss: 1.6492 - val_sparse_categorical_accuracy: 0.8114\n","Epoch 20/100\n","469/469 [==============================] - 11s 22ms/step - loss: 1.5469 - sparse_categorical_accuracy: 0.9142 - val_loss: 1.6746 - val_sparse_categorical_accuracy: 0.7862\n","Epoch 21/100\n","469/469 [==============================] - 11s 22ms/step - loss: 1.5533 - sparse_categorical_accuracy: 0.9078 - val_loss: 1.6593 - val_sparse_categorical_accuracy: 0.8016\n","Epoch 22/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5386 - sparse_categorical_accuracy: 0.9224 - val_loss: 1.6735 - val_sparse_categorical_accuracy: 0.7875\n","Epoch 23/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5439 - sparse_categorical_accuracy: 0.9172 - val_loss: 1.7447 - val_sparse_categorical_accuracy: 0.7164\n","Epoch 24/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5399 - sparse_categorical_accuracy: 0.9212 - val_loss: 1.6370 - val_sparse_categorical_accuracy: 0.8242\n","Epoch 25/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5457 - sparse_categorical_accuracy: 0.9154 - val_loss: 1.7212 - val_sparse_categorical_accuracy: 0.7396\n","Epoch 26/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5456 - sparse_categorical_accuracy: 0.9154 - val_loss: 1.5987 - val_sparse_categorical_accuracy: 0.8624\n","Epoch 27/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5452 - sparse_categorical_accuracy: 0.9158 - val_loss: 1.6434 - val_sparse_categorical_accuracy: 0.8176\n","Epoch 28/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5409 - sparse_categorical_accuracy: 0.9202 - val_loss: 1.6407 - val_sparse_categorical_accuracy: 0.8203\n","Epoch 29/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5353 - sparse_categorical_accuracy: 0.9258 - val_loss: 1.6083 - val_sparse_categorical_accuracy: 0.8529\n","Epoch 30/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5340 - sparse_categorical_accuracy: 0.9270 - val_loss: 1.7416 - val_sparse_categorical_accuracy: 0.7196\n","Epoch 31/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5472 - sparse_categorical_accuracy: 0.9139 - val_loss: 1.7280 - val_sparse_categorical_accuracy: 0.7326\n","Epoch 32/100\n","469/469 [==============================] - 10s 22ms/step - loss: 1.5362 - sparse_categorical_accuracy: 0.9249 - val_loss: 1.6033 - val_sparse_categorical_accuracy: 0.8579\n","Epoch 33/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5513 - sparse_categorical_accuracy: 0.9098 - val_loss: 1.6248 - val_sparse_categorical_accuracy: 0.8359\n","Epoch 34/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5401 - sparse_categorical_accuracy: 0.9210 - val_loss: 1.6037 - val_sparse_categorical_accuracy: 0.8573\n","Epoch 35/100\n","469/469 [==============================] - 10s 22ms/step - loss: 1.5225 - sparse_categorical_accuracy: 0.9386 - val_loss: 1.5880 - val_sparse_categorical_accuracy: 0.8732\n","Epoch 36/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5214 - sparse_categorical_accuracy: 0.9396 - val_loss: 1.6392 - val_sparse_categorical_accuracy: 0.8216\n","Epoch 37/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5566 - sparse_categorical_accuracy: 0.9044 - val_loss: 1.6091 - val_sparse_categorical_accuracy: 0.8520\n","Epoch 38/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5325 - sparse_categorical_accuracy: 0.9286 - val_loss: 1.6208 - val_sparse_categorical_accuracy: 0.8402\n","Epoch 39/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5254 - sparse_categorical_accuracy: 0.9356 - val_loss: 1.6164 - val_sparse_categorical_accuracy: 0.8444\n","Epoch 40/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5311 - sparse_categorical_accuracy: 0.9300 - val_loss: 1.6061 - val_sparse_categorical_accuracy: 0.8550\n","Epoch 41/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5248 - sparse_categorical_accuracy: 0.9362 - val_loss: 1.6008 - val_sparse_categorical_accuracy: 0.8602\n","Epoch 42/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5268 - sparse_categorical_accuracy: 0.9343 - val_loss: 1.5929 - val_sparse_categorical_accuracy: 0.8681\n","Epoch 43/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5215 - sparse_categorical_accuracy: 0.9396 - val_loss: 1.6282 - val_sparse_categorical_accuracy: 0.8331\n","Epoch 44/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5197 - sparse_categorical_accuracy: 0.9414 - val_loss: 1.5844 - val_sparse_categorical_accuracy: 0.8768\n","Epoch 45/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5150 - sparse_categorical_accuracy: 0.9462 - val_loss: 1.5993 - val_sparse_categorical_accuracy: 0.8617\n","Epoch 46/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5367 - sparse_categorical_accuracy: 0.9244 - val_loss: 1.6000 - val_sparse_categorical_accuracy: 0.8609\n","Epoch 47/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5289 - sparse_categorical_accuracy: 0.9322 - val_loss: 1.6008 - val_sparse_categorical_accuracy: 0.8602\n","Epoch 48/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5234 - sparse_categorical_accuracy: 0.9377 - val_loss: 1.6027 - val_sparse_categorical_accuracy: 0.8583\n","Epoch 49/100\n","469/469 [==============================] - 11s 24ms/step - loss: 1.5263 - sparse_categorical_accuracy: 0.9348 - val_loss: 1.6259 - val_sparse_categorical_accuracy: 0.8352\n","Epoch 50/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5292 - sparse_categorical_accuracy: 0.9318 - val_loss: 1.6406 - val_sparse_categorical_accuracy: 0.8205\n","Epoch 51/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5492 - sparse_categorical_accuracy: 0.9119 - val_loss: 1.6156 - val_sparse_categorical_accuracy: 0.8452\n","Epoch 52/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5265 - sparse_categorical_accuracy: 0.9346 - val_loss: 1.6015 - val_sparse_categorical_accuracy: 0.8595\n","Epoch 53/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5253 - sparse_categorical_accuracy: 0.9359 - val_loss: 1.6163 - val_sparse_categorical_accuracy: 0.8448\n","Epoch 54/100\n","469/469 [==============================] - 11s 24ms/step - loss: 1.5244 - sparse_categorical_accuracy: 0.9366 - val_loss: 1.5882 - val_sparse_categorical_accuracy: 0.8729\n","Epoch 55/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5162 - sparse_categorical_accuracy: 0.9449 - val_loss: 1.5875 - val_sparse_categorical_accuracy: 0.8734\n","Epoch 56/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5159 - sparse_categorical_accuracy: 0.9452 - val_loss: 1.5911 - val_sparse_categorical_accuracy: 0.8698\n","Epoch 57/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5117 - sparse_categorical_accuracy: 0.9494 - val_loss: 1.5702 - val_sparse_categorical_accuracy: 0.8909\n","Epoch 58/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5182 - sparse_categorical_accuracy: 0.9429 - val_loss: 1.5850 - val_sparse_categorical_accuracy: 0.8760\n","Epoch 59/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5238 - sparse_categorical_accuracy: 0.9373 - val_loss: 1.6202 - val_sparse_categorical_accuracy: 0.8407\n","Epoch 60/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5165 - sparse_categorical_accuracy: 0.9446 - val_loss: 1.6262 - val_sparse_categorical_accuracy: 0.8348\n","Epoch 61/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5231 - sparse_categorical_accuracy: 0.9380 - val_loss: 1.6192 - val_sparse_categorical_accuracy: 0.8419\n","Epoch 62/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5145 - sparse_categorical_accuracy: 0.9466 - val_loss: 1.5733 - val_sparse_categorical_accuracy: 0.8878\n","Epoch 63/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5214 - sparse_categorical_accuracy: 0.9398 - val_loss: 1.5935 - val_sparse_categorical_accuracy: 0.8676\n","Epoch 64/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5239 - sparse_categorical_accuracy: 0.9372 - val_loss: 1.5913 - val_sparse_categorical_accuracy: 0.8698\n","Epoch 65/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5147 - sparse_categorical_accuracy: 0.9464 - val_loss: 1.5789 - val_sparse_categorical_accuracy: 0.8822\n","Epoch 66/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5189 - sparse_categorical_accuracy: 0.9421 - val_loss: 1.5723 - val_sparse_categorical_accuracy: 0.8888\n","Epoch 67/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5147 - sparse_categorical_accuracy: 0.9463 - val_loss: 1.6128 - val_sparse_categorical_accuracy: 0.8481\n","Epoch 68/100\n","469/469 [==============================] - 11s 24ms/step - loss: 1.5238 - sparse_categorical_accuracy: 0.9373 - val_loss: 1.5837 - val_sparse_categorical_accuracy: 0.8773\n","Epoch 69/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5132 - sparse_categorical_accuracy: 0.9479 - val_loss: 1.5868 - val_sparse_categorical_accuracy: 0.8741\n","Epoch 70/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5169 - sparse_categorical_accuracy: 0.9442 - val_loss: 1.5760 - val_sparse_categorical_accuracy: 0.8849\n","Epoch 71/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5090 - sparse_categorical_accuracy: 0.9521 - val_loss: 1.5684 - val_sparse_categorical_accuracy: 0.8925\n","Epoch 72/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5166 - sparse_categorical_accuracy: 0.9445 - val_loss: 1.5836 - val_sparse_categorical_accuracy: 0.8775\n","Epoch 73/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5126 - sparse_categorical_accuracy: 0.9485 - val_loss: 1.5799 - val_sparse_categorical_accuracy: 0.8810\n","Epoch 74/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5121 - sparse_categorical_accuracy: 0.9489 - val_loss: 1.5849 - val_sparse_categorical_accuracy: 0.8762\n","Epoch 75/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5165 - sparse_categorical_accuracy: 0.9446 - val_loss: 1.6070 - val_sparse_categorical_accuracy: 0.8543\n","Epoch 76/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5271 - sparse_categorical_accuracy: 0.9341 - val_loss: 1.6136 - val_sparse_categorical_accuracy: 0.8473\n","Epoch 77/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5163 - sparse_categorical_accuracy: 0.9447 - val_loss: 1.5766 - val_sparse_categorical_accuracy: 0.8842\n","Epoch 78/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5110 - sparse_categorical_accuracy: 0.9501 - val_loss: 1.5784 - val_sparse_categorical_accuracy: 0.8827\n","Epoch 79/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5064 - sparse_categorical_accuracy: 0.9547 - val_loss: 1.5716 - val_sparse_categorical_accuracy: 0.8895\n","Epoch 80/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5062 - sparse_categorical_accuracy: 0.9549 - val_loss: 1.5747 - val_sparse_categorical_accuracy: 0.8861\n","Epoch 81/100\n","469/469 [==============================] - 10s 22ms/step - loss: 1.5240 - sparse_categorical_accuracy: 0.9371 - val_loss: 1.5869 - val_sparse_categorical_accuracy: 0.8741\n","Epoch 82/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5320 - sparse_categorical_accuracy: 0.9292 - val_loss: 1.6031 - val_sparse_categorical_accuracy: 0.8580\n","Epoch 83/100\n","469/469 [==============================] - 11s 24ms/step - loss: 1.5154 - sparse_categorical_accuracy: 0.9457 - val_loss: 1.5663 - val_sparse_categorical_accuracy: 0.8949\n","Epoch 84/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5116 - sparse_categorical_accuracy: 0.9495 - val_loss: 1.5815 - val_sparse_categorical_accuracy: 0.8795\n","Epoch 85/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5181 - sparse_categorical_accuracy: 0.9431 - val_loss: 1.6023 - val_sparse_categorical_accuracy: 0.8586\n","Epoch 86/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5188 - sparse_categorical_accuracy: 0.9423 - val_loss: 1.5861 - val_sparse_categorical_accuracy: 0.8749\n","Epoch 87/100\n","469/469 [==============================] - 11s 24ms/step - loss: 1.5172 - sparse_categorical_accuracy: 0.9439 - val_loss: 1.5885 - val_sparse_categorical_accuracy: 0.8727\n","Epoch 88/100\n","469/469 [==============================] - 11s 24ms/step - loss: 1.5123 - sparse_categorical_accuracy: 0.9488 - val_loss: 1.5885 - val_sparse_categorical_accuracy: 0.8725\n","Epoch 89/100\n","469/469 [==============================] - 12s 25ms/step - loss: 1.5136 - sparse_categorical_accuracy: 0.9475 - val_loss: 1.5815 - val_sparse_categorical_accuracy: 0.8794\n","Epoch 90/100\n","469/469 [==============================] - 11s 24ms/step - loss: 1.5080 - sparse_categorical_accuracy: 0.9531 - val_loss: 1.5848 - val_sparse_categorical_accuracy: 0.8762\n","Epoch 91/100\n","469/469 [==============================] - 11s 24ms/step - loss: 1.5140 - sparse_categorical_accuracy: 0.9470 - val_loss: 1.5952 - val_sparse_categorical_accuracy: 0.8658\n","Epoch 92/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5074 - sparse_categorical_accuracy: 0.9538 - val_loss: 1.5727 - val_sparse_categorical_accuracy: 0.8885\n","Epoch 93/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5068 - sparse_categorical_accuracy: 0.9544 - val_loss: 1.5978 - val_sparse_categorical_accuracy: 0.8631\n","Epoch 94/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5080 - sparse_categorical_accuracy: 0.9532 - val_loss: 1.6007 - val_sparse_categorical_accuracy: 0.8603\n","Epoch 95/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5067 - sparse_categorical_accuracy: 0.9544 - val_loss: 1.5802 - val_sparse_categorical_accuracy: 0.8808\n","Epoch 96/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5139 - sparse_categorical_accuracy: 0.9471 - val_loss: 1.5999 - val_sparse_categorical_accuracy: 0.8611\n","Epoch 97/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5118 - sparse_categorical_accuracy: 0.9492 - val_loss: 1.5860 - val_sparse_categorical_accuracy: 0.8750\n","Epoch 98/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5065 - sparse_categorical_accuracy: 0.9546 - val_loss: 1.5672 - val_sparse_categorical_accuracy: 0.8939\n","Epoch 99/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5031 - sparse_categorical_accuracy: 0.9579 - val_loss: 1.5980 - val_sparse_categorical_accuracy: 0.8630\n","Epoch 100/100\n","469/469 [==============================] - 11s 23ms/step - loss: 1.5133 - sparse_categorical_accuracy: 0.9478 - val_loss: 1.5739 - val_sparse_categorical_accuracy: 0.8874\n","<tensorflow.python.keras.callbacks.History object at 0x7fc70c8d9780>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":433},"id":"MeEdb1uXZgnp","executionInfo":{"status":"error","timestamp":1607650402310,"user_tz":-420,"elapsed":851,"user":{"displayName":"Hoang Viet Nguyen","photoUrl":"","userId":"00365099228452340445"}},"outputId":"d9170b02-1a1b-45af-cbe0-df59058183cf"},"source":["import tensorflow as tf\r\n","import tensorflow_datasets as tfds\r\n","import keras\r\n","from tensorflow.keras import datasets, layers, models\r\n","import matplotlib.pyplot as plt\r\n","from tensorflow.keras.models import Model\r\n","\r\n","ds_train, ds_test = tfds.load(\r\n","    'kmnist',\r\n","    split=['train', 'test'],\r\n","    shuffle_files=True,\r\n","    as_supervised=True,\r\n",")\r\n","\r\n","def normalize_img(image, label):\r\n","  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\r\n","  return tf.cast(image, tf.float32) / 255., label\r\n","\r\n","ds_train = ds_train.map(\r\n","    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n","ds_train = ds_train.batch(128)\r\n","ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\r\n","\r\n","ds_test = ds_test.map(\r\n","    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n","ds_test = ds_test.batch(128)\r\n","ds_test = ds_test.cache()\r\n","ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\r\n","\r\n","model = VGG19((28,28,1),10)\r\n","model.compile(\r\n","    optimizer=tf.keras.optimizers.Adam(),\r\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n","    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\r\n",")\r\n","history = model.fit(\r\n","    ds_train,\r\n","    epochs=5,\r\n","    validation_data=ds_test\r\n",")\r\n","print(history)\r\n","\r\n","model.save('/content/drive/My Drive/Shopee/vgg19-kmnist.hdf5')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1811\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1812\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1813\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d_44/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](conv2d_254/Relu)' with input shapes: [?,1,1,512].","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-36a0f88c1dd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mds_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG19\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m model.compile(\n\u001b[1;32m     32\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-51c7d3839d05>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, k)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPool2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    219\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 926\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1115\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m               \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/pooling.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         data_format=conv_utils.convert_data_format(self.data_format, 4))\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mmax_pool\u001b[0;34m(value, ksize, strides, padding, data_format, name, input)\u001b[0m\n\u001b[1;32m   4519\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4520\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4521\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   4522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mmax_pool\u001b[0;34m(input, ksize, strides, padding, data_format, name)\u001b[0m\n\u001b[1;32m   5267\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[1;32m   5268\u001b[0m         \u001b[0;34m\"MaxPool\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5269\u001b[0;31m                    data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m   5270\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5271\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    742\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[1;32m    743\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    591\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m    592\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         compute_device)\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3483\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3484\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3485\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3486\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3487\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1973\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[0;32m-> 1975\u001b[0;31m                                 control_input_ops, op_def)\n\u001b[0m\u001b[1;32m   1976\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1977\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1813\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1815\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1817\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d_44/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](conv2d_254/Relu)' with input shapes: [?,1,1,512]."]}]}]}